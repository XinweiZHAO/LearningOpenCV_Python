{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LearningOpenCV-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2  # 导入cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Image Processing in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Changing Colorspaces\n",
    "1. 如何进行图像色彩空间转换，BGR；Gray；HSV\n",
    "2. 创建应用程序，在视频中提取一个彩色的对象\n",
    "3. 函数`cv2.cvtColor()`；`cv2.inRange()` 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Changing Color-space\n",
    "OpenCV中有超过150个可用的色彩空间转换方法\n",
    "```Python\n",
    "cv2.cvtColor(input_img, flag)  # flag determines the type of conversion.\n",
    "```\n",
    "对于**HSV**：Hue：[0,179]；  Saturation：[0,255] and Value： [0,255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = [i for i in dir(cv2) if i.startswith('COLOR_')]\n",
    "# print(flags) # 所有可用转化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Object Tracking\n",
    "现在我们知道如何将BGR图片转为HSV色彩空间，我们可以利用这个来提取颜色对象。在表示一个颜色上，HSV相对RGB要来的简单。在这里我们提取一个蓝色目标。打开摄像头后可以拿一个蓝色物体在拍摄区域内以验证程序。\n",
    "\n",
    "**Extract a colored object：**\n",
    "- 提取视频帧\n",
    "- 将提取的图片从RGB转为HSV\n",
    "- 通过提供一个蓝色值的范围来对图像做阈值（We threshold the HSV image for a range of blue color）\n",
    "- 现在我们可以提取出这个蓝色对象，然后可以做任何想做的事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while(1):\n",
    "    # Take each frame\n",
    "    _, frame = cap.read()\n",
    "    # Convert BGR to HSV\n",
    "    img_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    # define range of blue color in HSV\n",
    "    lower_blue = np.array([110,50,50])\n",
    "    upper_blue = np.array([130,255,255])\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    mask = cv2.inRange(img_hsv, lower_blue, upper_blue)\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(frame,frame, mask = mask)\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27: # Esc key\n",
    "        break\n",
    "cap.release() # release不可忘\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. How to find HSV values to track?\n",
    "既可以利用 `cv2.cvtColor` 将RGB颜色转为HSV,然后再利用转换后的颜色来提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 60 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "green = np.uint8([[[0,255,0 ]]])\n",
    "hsv_green = cv2.cvtColor(green,cv2.COLOR_BGR2HSV) # find the HSV value of Green\n",
    "print(hsv_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Image Thresholding\n",
    "**学会简单阈值，自适应阈值(Adaptive thresholding)，大津阈值(Otsu’s thresholding)等等。**\n",
    "\n",
    "主要函数: `cv2.threshold()和cv2.adaptiveThreshold()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Simple Thresholding\n",
    "图像阈值处理比较直白：如果像素值大于阈值时，它被重置为一个值（可以是white），否则它被赋予另一个值（可能是black）\n",
    "- `cv2.threshold()`函数 :\n",
    "  - 输入参数：\n",
    "    - 第一参数：img，a grayscale image\n",
    "    - 第二参数：阈值，用于分类像素值\n",
    "    - 第三参数maxVal，当像素值超过（sometimes less than）阈值时对该像素赋的值\n",
    "    - 第四参数，代表不同的thresholding类型:\n",
    "      - cv2.THRESH_BINARY\n",
    "      - cv2.THRESH_BINARY_INV\n",
    "      - cv2.THRESH_TRUNC\n",
    "      - cv2.THRESH_TOZERO\n",
    "      - cv2.THRESH_TOZERO_INV\n",
    "  - 输出参数：\n",
    "    - retval\n",
    "    - thresholded image\n",
    "    \n",
    "**代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/gradient.jpg',0)\n",
    "ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)\n",
    "ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC)\n",
    "ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO)\n",
    "ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV)\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "for i in xrange(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Adaptive Thresholding\n",
    "在前面的阈值方法中我们采用一个全局值（global value）作为唯一的阈值。\n",
    ">get different thresholds for different regions of the same image and it gives us better results for images with varying illumination（光照）.\n",
    "\n",
    "```Python\n",
    "Docstring: adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst]) -> dst\n",
    "```\n",
    "参数：\n",
    "- **Adaptive Method **- It decides how thresholding value is calculated.\n",
    "  - cv2.ADAPTIVE_THRESH_MEAN_C : threshold value is the mean of neighbourhood area.\n",
    "  - cv2.ADAPTIVE_THRESH_GAUSSIAN_C : threshold value is the weighted sum of neighbourhood values where weights are a gaussian window.\n",
    "- **Block Size** - It decides the size of neighbourhood area.\n",
    "\n",
    "- **C **- It is just a constant which is subtracted from the mean or weighted mean calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compares global thresholding and adaptive thresholding \n",
    "img = cv2.imread('./input/dave.jpg',0)\n",
    "img = cv2.medianBlur(img,5)\n",
    "\n",
    "ret,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "th2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "\n",
    "for i in xrange(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Otsu’s Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们使用大律阈值时就需要用到前面提到的阈值函数返回值**`retVal`**。在全局阈值法（global thresholding,即1. Simple Thresholding）中，我们采用了一个随机值来定义这个全局阈值，但我们不能确是好是坏，方法是不断地尝试。但是对于一个双峰图片（**bimodal image** , it's histogram has two peaks），我们则可以近似采用一个双峰的中间值。这就是大律法的处理方法。所以简单地说该方法自动基于图片双峰计算这个阈值（当然对于非双峰图片，则该方法不会准确）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里我们仍使用 ` cv2.threshold` 这个函数，但需要传入一个额外的标志 - `cv2.THRESH_OTSU`。**而对于阈值则简单传入一个0即可**。然后算法会自动计算并将最优阈值返回到**`retVal`**。如果大律阈值没有被使用，则**`retVal`**为你最初传入的那个简单阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/noisy2.png',0)\n",
    "# global thresholding\n",
    "ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in xrange(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Geometric Transformations of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/messi5.jpg')\n",
    "res = cv2.resize(img, None, fx=2, fy=2, interpolation = cv2.INTER_AREA)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('res',res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 或者通过如下方法：\n",
    "height, width = img.shape[:2]\n",
    "res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('res',res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "# define Translation Matrix = \n",
    "#   [ 1 0 tx \n",
    "#     0 1 ty ]\n",
    "M = np.float32([[1,0,200],[0,1,50]]) # 水平方向200 垂直方向50\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/messi5.jpg',0)\n",
    "rows,cols = img.shape\n",
    "# 获得变换矩阵\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),60,0.6) # 给定：center旋转中心, angle旋转角, scale缩放系数\n",
    "# 实施变换\n",
    "dst = cv2.warpAffine(img, M, (cols,rows)) # image, 变换Matrix, 图像size\n",
    "cv2.imshow('Rotateimg',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Affine Transformation (仿射变换)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/drawing.png')\n",
    "rows,cols,ch = img.shape\n",
    "# 变换对应3个点\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "# 获得变换矩阵\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "# 实施变换\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Perspective Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/sudokusmall.png')\n",
    "rows,cols,ch = img.shape\n",
    "# 定义变换对应的4个点\n",
    "pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n",
    "# 获得变换矩阵\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "# 实施变换\n",
    "dst = cv2.warpPerspective(img,M,(400,400))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Smoothing Images（图像平滑/滤波）\n",
    "图像平滑/滤波的目的是消除图像噪声\n",
    "1. 学习图像模糊，各种低通滤波器（邻域均值滤波averaging filtering or low pass filtering）\n",
    "2. 应用定制的过滤器（二维卷积）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 2D Convolution ( Image Filtering )\n",
    "对于一维信号，图像也可以通过利低通滤波器(LPF), 高通滤波器 (HPF)来进行滤波处理。LPF有利于消除噪声或者模糊图像。HPF则有助于发现图像中的一些边界。\n",
    "```Python \n",
    "cv2.filter2D() #  convolve a kernel with an image\n",
    "```\n",
    " 5x5 averaging filter kernel can be defined as follows:\n",
    "$$K =\\frac{1}{25}\\left(\n",
    "\\begin{array}{ccccc}\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "首先定义一个5x5的框，以图像中某一像素为中心放置该框，落入该框的所有像素值被求和，结果再除以25，以该值作为滤波后该像素的值，然后对图像中的每一像素都这么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/opencv_logo.png')\n",
    "# define 5x5 averaging filter kernel\n",
    "kernel = np.ones((5,5),np.float32)/25 \n",
    "# apply averaging filter\n",
    "dst = cv2.filter2D(img,-1,kernel) \n",
    "# plot results\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Averaging')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Image Blurring (Image Smoothing)\n",
    "图像模糊：通过原图与一个LPF卷积来获得。有助于消除噪声。实际上是消除了图像高频信息（例如噪声，边界角点等等），这样会消除图像中的边界信息，当然也有模糊方法其不消除边界信息。OpenCV主要提供了4种模糊技术。\n",
    "1. Averaging\n",
    "$$K = \\frac{1}{9}\\left(\n",
    "\\begin{array}{ccc}\n",
    " 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "通过与归一化的 box filter 卷积：`cv2.blur()` or `cv2.boxFilter()`\n",
    "若不想使用归一化的 box filter 则传入 `normalize=False `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/opencv_logo.png')\n",
    "blur = cv2.blur(img,(5,5))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Gaussian Filtering\n",
    "\n",
    "对于高斯噪声非常有效，需要定义高斯核：高度、宽度以及x与y方向的标准差，或者也可以采用函数 `cv2.getGaussianKernel()` 生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/opencv_logo.png')\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Median Filtering\n",
    "\n",
    "在核窗口下计算所有像素的中值，然后将中心像素值替换为该值。这是在去除椒盐噪声（随机噪声）非常有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/Median Filtering.png')\n",
    "median = cv2.medianBlur(img,7)\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(median),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bilateral Filtering（双边滤波）\n",
    "前面所述滤波器会消除边界信息，但双边滤波不会。对于消除噪声同时保留边界时是十分有效的。但处理速度相对其他滤波器要慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.subplot(121),plt.imshow(img),plt.title('Original')\\nplt.xticks([]), plt.yticks([])\\nplt.subplot(122),plt.imshow(blur),plt.title('Blurred')\\nplt.xticks([]), plt.yticks([])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('./input/bilateral.jpg')\n",
    "blur = cv2.bilateralFilter(img,19,75,75)\n",
    "# 表面质感消失了，但边缘仍保留。\n",
    "cv2.imshow('Bilateral Filtering',blur)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.5 Morphological Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/MorphTrans.png',0)\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "kernel2 = np.ones((8,8),np.uint8)\n",
    "erosion = cv2.erode(img,kernel,iterations = 1)\n",
    "erosion2 = cv2.erode(img,kernel2,iterations = 1)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('erosion',erosion)\n",
    "cv2.imshow('erosion2',erosion2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/MorphTrans.png',0)\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "kernel2 = np.ones((8,8),np.uint8)\n",
    "dilation = cv2.dilate(img,kernel,iterations = 1)\n",
    "dilation2 = cv2.dilate(img,kernel2,iterations = 1)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('erosion',dilation)\n",
    "cv2.imshow('erosion2',dilation2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Opening(Erosion followed by Dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/opening.png',0)\n",
    "kernel = np.ones((6,6),np.uint8)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('erosion',opening)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Closing(Dilation followed by Erosion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/closing.png',0)\n",
    "kernel = np.ones((6,6),np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('erosion',closing)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Morphological Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/MorphTrans.png',0)\n",
    "kernel = np.ones((6,6),np.uint8)\n",
    "gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('erosion',gradient)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  Top Hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/MorphTrans.png',0)\n",
    "kernel = np.ones((12,12),np.uint8)\n",
    "tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('opening',opening)\n",
    "cv2.imshow('erosion',tophat)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Black Hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/MorphTrans.png',0)\n",
    "kernel = np.ones((20,20),np.uint8)\n",
    "blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('closing',closing)\n",
    "cv2.imshow('erosion',blackhat)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring Element\n",
    "need elliptical/circular shaped kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.getStructuringElement(cv2.MORPH_CROSS,(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.4.6 Image Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sobel and Scharr Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/dave.jpg',0)\n",
    "\n",
    "laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n",
    "sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n",
    "\n",
    "plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Important Matter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./input/double_edge.jpg',0)\n",
    "# Output dtype = cv2.CV_8U\n",
    "sobelx8u = cv2.Sobel(img,cv2.CV_8U,1,0,ksize=7)\n",
    "# Output dtype = cv2.CV_64F. Then take its absolute and convert to cv2.CV_8U\n",
    "sobelx64f = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=1)\n",
    "abs_sobel64f = np.absolute(sobelx64f)\n",
    "sobel_8u = np.uint8(abs_sobel64f)\n",
    "# Black-to-White transition is taken as Positive slope (it has a positive value) \n",
    "# while White-to-Black transition is taken as a Negative slope (It has negative value). \n",
    "# So when you convert data to np.uint8, all negative slopes are made zero. \n",
    "# CV_8U只有左侧Black-to-White transition一边\n",
    "plt.subplot(1,3,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(1,3,2),plt.imshow(sobelx8u,cmap = 'gray')\n",
    "plt.title('Sobel CV_8U'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(1,3,3),plt.imshow(sobel_8u,cmap = 'gray')\n",
    "plt.title('Sobel abs(CV_64F)'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.7 Canny Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/messi5.jpg',0)\n",
    "edges = cv2.Canny(img,100,200, 5, L2gradient = True)\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.8 Image Pyramids\n",
    "1. Gaussian Pyramid  \n",
    "2. Laplacian Pyramids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('../Ch3_Core_Operations/input/messi5.jpg')\n",
    "lower_reso = cv2.pyrDown(img)\n",
    "higher_reso = cv2.pyrUp(img)\n",
    "cv2.imshow('img',img)\n",
    "cv2.imshow('lower_reso',lower_reso)\n",
    "cv2.imshow('higher_reso',higher_reso)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-9462c214ceba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyrDown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mgpA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# generate Gaussian pyramid for B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "A = cv2.imread('./input/apple.jpg')\n",
    "B = cv2.imread('./input/orange.jpg')\n",
    "# generate Gaussian pyramid for A\n",
    "G = A.copy()\n",
    "gpA = [G]\n",
    "for i in xrange(6):\n",
    "    G = cv2.pyrDown(G)\n",
    "    gpA.append(G)\n",
    "# generate Gaussian pyramid for B\n",
    "G = B.copy()\n",
    "gpB = [G]\n",
    "for i in xrange(6):\n",
    "    G = cv2.pyrDown(G)\n",
    "    gpB.append(G)\n",
    "# generate Laplacian Pyramid for A\n",
    "lpA = [gpA[5]]\n",
    "for i in xrange(5,0,-1):\n",
    "    GE = cv2.pyrUp(gpA[i])\n",
    "    L = cv2.subtract(gpA[i-1],GE)\n",
    "    lpA.append(L)\n",
    "# generate Laplacian Pyramid for B\n",
    "lpB = [gpB[5]]\n",
    "for i in xrange(5,0,-1):\n",
    "    GE = cv2.pyrUp(gpB[i])\n",
    "    L = cv2.subtract(gpB[i-1],GE)\n",
    "    lpB.append(L)\n",
    "# Now add left and right halves of images in each level\n",
    "LS = []\n",
    "for la,lb in zip(lpA,lpB):\n",
    "    rows,cols,dpt = la.shape\n",
    "    ls = np.hstack((la[:,0:cols/2], lb[:,cols/2:]))\n",
    "    LS.append(ls)\n",
    "# now reconstruct\n",
    "ls_ = LS[0]\n",
    "for i in xrange(1,6):\n",
    "    ls_ = cv2.pyrUp(ls_)\n",
    "    ls_ = cv2.add(ls_, LS[i])\n",
    "\n",
    "# image with direct connecting each half\n",
    "real = np.hstack((A[:,:cols/2],B[:,cols/2:]))\n",
    "\n",
    "cv2.imwrite('./output/Pyramid_blending2.jpg',ls_)\n",
    "cv2.imwrite('./output/Direct_blending.jpg',real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
